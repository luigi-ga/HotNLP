{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "# Pearson, Spearman\n",
    "import scipy\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "\n",
    "# Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Explicit representation\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/luigi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/luigi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/luigi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/luigi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the results directory if it does not exist\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "\n",
    "# Create the model directory if it does not exist\n",
    "if not os.path.exists('model'):\n",
    "    os.mkdir('model')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  dataset.zip\n",
      "Archive:  ./data/sample_annotated_sentences.zip\n",
      "Archive:  ./data/semantic_simlex_v0.1.zip\n",
      "Archive:  ./data/student_prediciton_example.zip\n"
     ]
    }
   ],
   "source": [
    "!unzip -n dataset.zip -d ./data\n",
    "!unzip -n ./data/sample_annotated_sentences.zip -d ./data/sample_annotated_sentences\n",
    "!unzip -n ./data/semantic_simlex_v0.1.zip -d ./data/semantic_simlex_v0.1\n",
    "! unzip -n ./data/student_prediciton_example.zip -d ./data/student_prediciton_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOSAICO Dataset Preprocessing\n",
    "\n",
    "This section of the notebook is dedicated to preprocessing the MOSAICO dataset, a substantial collection of words with assigned WordNet meanings. The primary objective is to refine the dataset for semantic analysis by standardizing word forms and filtering for semantically relevant content.\n",
    "\n",
    "The preprocessing begins by defining custom lemmas. This step is crucial to ensure words in different forms, such as 'people' and 'stolen', are uniformly represented as 'person' and 'steal', respectively. It aids in maintaining consistency across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some custom lemmas\n",
    "custom_lemmas = {\n",
    "    'people': 'person',\n",
    "    'stolen': 'steal',\n",
    "    'quicker': 'quick',\n",
    "    'shrunk': 'shrink',\n",
    "    'leaf': 'leave',\n",
    "    'hung': 'hang',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion of the part-of-speech tags from the Penn Treebank tagset to the WordNet tagset is provided, which is essential for accurate WordNet-based lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert part-of-speech tags from the Penn Treebank tagset to the WordNet tagset.\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `process_jsonl_file` function is at the core of our preprocessing workflow. It processes a JSONL file of the dataset, where each line is tokenized, POS-tagged, and lemmatized using both custom and standard WordNet lemmatization. \n",
    "A critical aspect of this function is the evaluation of sentences based on their annotations. In this step, we aim to replace specific words with their corresponding senses (as specified in the annotations). However, to ensure the reliability of the data, we first verify that the lemmatized form of a word is indeed present in the intended sense. If the lemma does not match the expected sense, the sentence is discarded. This procedure helps in filtering out potential errors in the dataset, ensuring that only accurately annotated sentences are retained for semantic analysis.\n",
    "\n",
    "The processed data is segregated into two categories: semantic and non-semantic sentences. Semantic sentences comprise those that have been successfully lemmatized and annotated with the correct senses, whereas non-semantic sentences include those filtered out due to mismatches or stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a JSONL file and return the semantic and non-semantic sentences tokens\n",
    "def process_jsonl_file(filepath, custom_lemmas, limit=None):\n",
    "    # Initialize a lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Initialize a spacy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Initialize some variables\n",
    "    semantic_sentences = []\n",
    "    non_semantic_sentences = []\n",
    "    discarded_indices = []\n",
    "    line_counter = 0\n",
    "\n",
    "    # Define a set of stopwords for performance\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Include punctuation in the set of stopwords\n",
    "    stop_words.update(set(string.punctuation))\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            # Stop if limit is reached\n",
    "            if limit is not None and line_counter >= limit: break\n",
    "                \n",
    "            # Load the JSON line and tokenize the sentence\n",
    "            data = json.loads(line)\n",
    "            tokens = data['text'].lower().split(' ')\n",
    "            # Get the annotations\n",
    "            annotations = data.get('annotations', [])\n",
    "            \n",
    "            # POS tagging\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            # Lemmatize the tokens and prepare for annotations\n",
    "            lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag) or wordnet.NOUN) for token, tag in pos_tags]\n",
    "\n",
    "            # Initialize a flag to indicate if the sentence should be discarded\n",
    "            discard_sentence = False\n",
    "\n",
    "            # Iterate through the annotations\n",
    "            for annotation in annotations:\n",
    "                # Get the token span and label\n",
    "                token_span = annotation['token_span']\n",
    "                label = annotation['label']\n",
    "\n",
    "                # Check if the token span is valid\n",
    "                if token_span[1] <= len(lemmatized_tokens):\n",
    "                    # Check if the lemma at the token span matches the label\n",
    "                    lemma = lemmatized_tokens[token_span[0]]\n",
    "\n",
    "                    # If the label is a substring of the lemma, it is valid\n",
    "                    if lemma in label:\n",
    "                        lemmatized_tokens[token_span[0]] = label\n",
    "\n",
    "                    # Try with spacy lemmatization\n",
    "                    elif nlp(tokens[token_span[0]])[0].lemma_ in label:\n",
    "                        lemmatized_tokens[token_span[0]] = label\n",
    "\n",
    "                    # Try with custom lemmatization\n",
    "                    elif lemma in custom_lemmas and custom_lemmas[lemma] in label:\n",
    "                        lemmatized_tokens[token_span[0]] = label\n",
    "                        \n",
    "                    # If the lemma does not match the label, discard the sentence\n",
    "                    else:\n",
    "                        if DEBUG:\n",
    "                            print(f\"\\n{data['text']}' at line {i+1}\")\n",
    "                            print(lemmatized_tokens)\n",
    "                            print(f\"Error at index {token_span[0]}, label '{label}', token '{lemma}'\")\n",
    "                        discard_sentence = True\n",
    "                        discarded_indices.append(i)\n",
    "                        # Break the loop (stop checking the annotations)\n",
    "                        break\n",
    "\n",
    "            # If the sentence should not be discarded\n",
    "            if not discard_sentence:\n",
    "                # Append the semantic sentences\n",
    "                semantic_sentences.append([token for token in lemmatized_tokens if token not in stop_words])\n",
    "            \n",
    "            # Append the non-semantic sentences\n",
    "            non_semantic_sentences.append([token for token in tokens if token not in stop_words])\n",
    "\n",
    "            # Increment the line counter\n",
    "            line_counter += 1\n",
    "\n",
    "    # Save the discarded indices\n",
    "    with open('./results/discarded_indices.json', 'w') as file:\n",
    "        json.dump(discarded_indices, file)\n",
    "    \n",
    "    # Return the processed sentences\n",
    "    return semantic_sentences, non_semantic_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system also checks for existing processed data, loading it if available, or processes and saves new data if necessary. This approach enhances efficiency by avoiding repetitive processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data to ./results/non_semantic_data.json\n",
      "Total discarded semantic sentences: 41500 (7.77%)\n",
      "Total discarded non semantic sentences: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "semantic_data_path = './results/semantic_data.json'\n",
    "non_semantic_data_path = './results/non_semantic_data.json'\n",
    "\n",
    "# Load the processed data if it exists\n",
    "if os.path.exists(semantic_data_path) and os.path.exists(non_semantic_data_path):\n",
    "    with open(semantic_data_path, 'r') as file:\n",
    "        semantic_sentences = json.load(file)\n",
    "    with open(non_semantic_data_path, 'r') as file:\n",
    "        non_semantic_sentences = json.load(file)\n",
    "    print(f\"Loaded processed data from {non_semantic_data_path}\")\n",
    "\n",
    "# Otherwise, process the sample annotated sentences and save to a JSON file\n",
    "else:\n",
    "    semantic_sentences, non_semantic_sentences = process_jsonl_file('data/sample_annotated_sentences/500000.jsonl', custom_lemmas, limit=None)\n",
    "    with open(semantic_data_path, 'w') as file:\n",
    "        json.dump(semantic_sentences, file)\n",
    "    with open(non_semantic_data_path, 'w') as file:\n",
    "        json.dump(non_semantic_sentences, file)\n",
    "    print(f\"Saved processed data to {non_semantic_data_path}\")\n",
    "\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Total discarded semantic sentences: {534300 - len(semantic_sentences)} ({(534300 - len(semantic_sentences)) / 534300 * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Training and Word Similarity Analysis\n",
    "\n",
    "This section of the notebook focuses on training Word2Vec models using the preprocessed data for both non-semantic and semantic sentences. The models are tailored with specific parameters, including vector size, window size, and number of epochs, to effectively capture the intricate relationships within the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "nsm_model = Word2Vec(\n",
    "    non_semantic_sentences,\n",
    "    vector_size=100,  # Vector size\n",
    "    window=5,         # A larger window for more context\n",
    "    min_count=1,      # Ignore words that appear less than 1 time\n",
    "    workers=4,        # Match to the number of available CPU cores\n",
    "    epochs=50,        # More epochs for better convergence\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "nsm_model.train(non_semantic_sentences, total_examples=nsm_model.corpus_count, epochs=nsm_model.epochs)\n",
    "\n",
    "# Save the model and the embeddings\n",
    "nsm_model_path = f\"model/nsm_embeddings_{nsm_model.epochs}ep.model\"\n",
    "nsm_model.save(nsm_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "sm_model = Word2Vec(\n",
    "    semantic_sentences,\n",
    "    vector_size=100,  # Consider trying larger sizes\n",
    "    window=5,         # A larger window for more context\n",
    "    min_count=1,      # Ignore words that appear less than 1 times\n",
    "    workers=4,        # Match to the number of available CPU cores\n",
    "    epochs=50,        # More epochs for better convergence\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "sm_model.train(semantic_sentences, total_examples=nsm_model.corpus_count, epochs=nsm_model.epochs)\n",
    "\n",
    "# Save the model and the embeddings\n",
    "sm_model_path = f\"model/sm_embeddings_{sm_model.epochs}ep.model\"\n",
    "sm_model.save(sm_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-training, a unified function `compute_correlation_scores` is employed to compute correlation scores between word pairs. This function is a key component in evaluating the models' ability to replicate human-like word similarity assessments. It operates in two modes: 'non_semantic' and 'semantic', each addressing a distinct aspect of language understanding.\n",
    "\n",
    "In 'non_semantic' mode, the function evaluates word pairs in their plain form, directly comparing their similarity against human-rated scores. This mode is crucial for understanding how the model perceives relationships between words without the influence of semantic context.\n",
    "\n",
    "Conversely, in 'semantic' mode, the function acknowledges the multiple senses of words. It computes the maximum similarity across all possible combinations of senses for each word pair. This is particularly important for assessing the model's proficiency in capturing the subtleties of semantic meanings.\n",
    "\n",
    "For both modes, the function iterates over word pairs, checks their presence in the model's vocabulary, and calculates similarity scores. It handles cases where valid word pairs are not found by appending a default score and can issue warnings for such instances.\n",
    "\n",
    "The performance of the models in both semantic and non-semantic contexts is quantified using Pearson's r and Spearman's rho correlation coefficients. These metrics provide a comprehensive view of the models' alignment with human perception of word similarity, thus offering insights into the complexities of language understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation scores\n",
    "def compute_correlation_scores(model, word_pair2score, mode, print_warning=DEBUG):\n",
    "    # Initialize lists to store the scores\n",
    "    human_scores = []\n",
    "    system_scores = []\n",
    "\n",
    "    # Iterate through the word pairs\n",
    "    for word_pair, score in word_pair2score.items():\n",
    "        # Initialize variables\n",
    "        max_similarity = -1\n",
    "        found_pair = False\n",
    "\n",
    "        # Get the word pairs\n",
    "        if mode == 'non_semantic':\n",
    "            w1, w2 = word_pair          \n",
    "            word_pairs = [(w1, w2)]\n",
    "        elif mode == 'semantic':\n",
    "            w1_senses, w2_senses = word_pair\n",
    "            word_pairs = product(w1_senses.split(','), w2_senses.split(','))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'non_semantic' or 'semantic'.\")\n",
    "\n",
    "        # Iterate through the word pairs\n",
    "        for w1, w2 in word_pairs:\n",
    "            # Check if the word pair is in the model\n",
    "            if w1 in model.wv.key_to_index and w2 in model.wv.key_to_index:\n",
    "                # Compute the similarity\n",
    "                system_similarity = model.wv.similarity(w1, w2)\n",
    "                # Update the max similarity\n",
    "                max_similarity = max(max_similarity, system_similarity)\n",
    "                # Update the flag\n",
    "                found_pair = True\n",
    "\n",
    "        # If the word pair was not found, append -1\n",
    "        if not found_pair:\n",
    "            if print_warning: \n",
    "                print(f\"WARNING: No valid word pairs for {word_pair} were found in the embedding model.\")\n",
    "            system_scores.append(-1)    \n",
    "        # Otherwise, append the max similarity if in semantic mode       \n",
    "        elif found_pair and mode == 'semantic':\n",
    "            system_scores.append(max_similarity)\n",
    "        # Otherwise, append the system similarity if in non-semantic mode\n",
    "        elif found_pair and mode == 'non_semantic':\n",
    "            system_scores.append(system_similarity)\n",
    "        # Append the human score\n",
    "        human_scores.append(score)\n",
    "\n",
    "    # Convert lists to numpy arrays for the pairs that were found\n",
    "    if human_scores and system_scores:\n",
    "        human_scores = np.array(human_scores)\n",
    "        system_scores = np.array(system_scores)\n",
    "\n",
    "        # Compute correlation\n",
    "        pearson_r, _ = scipy.stats.pearsonr(human_scores, system_scores)\n",
    "        spearman_rho = scipy.stats.spearmanr(human_scores, system_scores).correlation\n",
    "    else:\n",
    "        pearson_r, spearman_rho = float('nan'), float('nan')\n",
    "\n",
    "    # Return the correlation scores\n",
    "    return pearson_r, spearman_rho\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, this analysis provides a dual perspective on word similarity: one that considers the plain form of words and another that delves into their deeper semantic meanings. By comparing these two approaches, we gain valuable insights into the complexities of language understanding and the efficacy of Word2Vec models in different linguistic contexts.\n",
    "\n",
    "Through this dual-mode analysis, we gain a deeper understanding of word similarities, both in the context of plain word forms and their deeper semantic meanings. This approach allows for a more nuanced evaluation of the Word2Vec models in capturing the essence of word relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NON SEMANTIC score \n",
      "Pearson's r: 0.3726, Spearman's rho: 0.3592\n",
      "\n",
      "SEMANTIC score: \n",
      "Pearson's r: 0.3390, Spearman's rho: 0.3216\n"
     ]
    }
   ],
   "source": [
    "# Load the TSV file\n",
    "simlex999_path = './data/semantic_simlex_v0.1/semantic_simlex_v0.1.tsv'\n",
    "simlex999 = pd.read_csv(simlex999_path, delimiter='\\t')\n",
    "\n",
    "# Extract sense-annotated word pairs and their scores\n",
    "nsm_word_pair2score = {(row['word1'], row['word2']): row['SimLex999'] for _, row in simlex999.iterrows()}\n",
    "\n",
    "# Compute the correlation score and print the results\n",
    "nsm_pearson_r, nsm_spearman_rho = compute_correlation_scores(nsm_model, nsm_word_pair2score, mode='non_semantic')\n",
    "print(f\"NON SEMANTIC score \\nPearson's r: {nsm_pearson_r:.4f}, Spearman's rho: {nsm_spearman_rho:.4f}\\n\")\n",
    "\n",
    "\n",
    "# Extract sense-annotated word pairs and their scores\n",
    "sm_word_pair2score = {(row['Senses(w1)'], row['Senses(w2)']): row['SimLex999'] for _, row in simlex999.iterrows()}\n",
    "\n",
    "# Compute the correlation score and print the results\n",
    "sm_pearson_r, sm_spearman_rho = compute_correlation_scores(sm_model, sm_word_pair2score, mode='semantic')\n",
    "print(f\"SEMANTIC score: \\nPearson's r: {sm_pearson_r:.4f}, Spearman's rho: {sm_spearman_rho:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Similarity Computation and Analysis on SimLex-999 Dataset\n",
    "\n",
    "This part of the notebook focuses on computing the maximum similarity between word pairs from the SimLex-999 dataset, using the previously trained Word2Vec models for semantic and non-semantic contexts. The aim is to assess how well these models capture the relationships between words as reflected in human-rated similarity scores.\n",
    "\n",
    "The `compute_max_similarity` function is at the core of this analysis. It operates in two modes: 'semantic' and 'non-semantic'. In the 'semantic' mode, the function considers multiple senses of words, iterating through all possible sense combinations to find the highest similarity score. In contrast, the 'non-semantic' mode evaluates the similarity between words in their plain form, without considering different senses.\n",
    "\n",
    "The function iterates through each word pair, checking if they exist in the model's vocabulary. If both words are found, it computes their similarity and updates the maximum similarity score. A warning is printed if a word pair is not found in the model, ensuring transparency in the analysis process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comute max similarity\n",
    "def compute_max_similarity(model, row, mode='semantic', print_warning=DEBUG):\n",
    "    # Initialize the max similarity\n",
    "    max_similarity = -1\n",
    "\n",
    "    # Get the word pairs\n",
    "    if mode == 'semantic':\n",
    "        w1_senses = row['Senses(w1)'].split(',')\n",
    "        w2_senses = row['Senses(w2)'].split(',')\n",
    "        pairs = product(w1_senses, w2_senses)\n",
    "    elif mode == 'non_semantic':\n",
    "        pairs = [(row['word1'], row['word2'])]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose 'semantic' or 'non_semantic'.\")\n",
    "\n",
    "    # Iterate through the word pairs\n",
    "    for w1, w2 in pairs:\n",
    "        # Check if the word pair is in the model\n",
    "        if w1 in model.wv.key_to_index and w2 in model.wv.key_to_index:\n",
    "            # Compute the similarity\n",
    "            similarity = model.wv.similarity(w1, w2)\n",
    "            # Update the max similarity\n",
    "            max_similarity = max(max_similarity, similarity)\n",
    "\n",
    "        # Otherwise, print a warning\n",
    "        elif print_warning:\n",
    "            print(f\"WARNING: ({w1} and {w2}) are not present in the embedding model.\")\n",
    "            \n",
    "    # Return the max similarity \n",
    "    return max_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the similarity computation, the `process_simlex` function processes the SimLex-999 dataset. It calculates the maximum similarity for each word pair in the dataset, in either 'semantic' or 'non-semantic' mode as specified. The results are stored in a DataFrame, capturing the predicted similarity scores for each word pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process SimLex-999\n",
    "def process_simlex(file_path, model, mode='semantic'):\n",
    "    # Load the TSV file\n",
    "    df = pd.read_csv(file_path, delimiter='\\t')\n",
    "    # Initialize a list to store the results\n",
    "    results = []\n",
    "\n",
    "    # Iterate through the rows\n",
    "    for index, row in df.iterrows():\n",
    "        # Compute the max similarity\n",
    "        max_similarity = compute_max_similarity(model, row, mode=mode)\n",
    "        # Append the results\n",
    "        results.append({\n",
    "            'Word1': row['word1'],\n",
    "            'Word2': row['word2'],\n",
    "            'Predicted_Similarity': max_similarity if max_similarity != -1 else 'N/A'\n",
    "        })\n",
    "\n",
    "    # Return the results\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the results are saved into separate TSV files for non-semantic and semantic analyses. This step provides an organized and accessible record of the model's performance in both contexts. The separate files, 'non_semantic.tsv' and 'semantic.tsv', offer a clear comparison of how the models perform in understanding word similarities in different linguistic scenarios.\n",
    "\n",
    "By analyzing the maximum similarity in these two distinct contexts, we gain valuable insights into the capabilities and limitations of our Word2Vec models in capturing the essence of word relationships as perceived by humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the results to non_semantic.tsv\n",
      "Saved the results to semantic.tsv\n"
     ]
    }
   ],
   "source": [
    "# Process SimLex-999 for non-semantic\n",
    "nsm_df = process_simlex(simlex999_path, nsm_model, mode='non_semantic')\n",
    "nsm_df.to_csv('./results/non_semantic.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"Saved the results to non_semantic.tsv\")\n",
    "\n",
    "# Process SimLex-999 for semantic\n",
    "sm_df = process_simlex(simlex999_path, sm_model, mode='semantic')\n",
    "sm_df.to_csv('./results/semantic.tsv', sep='\\t', index=False, header=False)\n",
    "print(\"Saved the results to semantic.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally proceed to export the 5 most difficult pairs imo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pairs and comments\n",
    "lines = [\n",
    "    (\"happy\", \"young\"),\n",
    "    (\"'happy' is an emotional state, while 'young' is a descriptor of age or development stage.\", \"\"),\n",
    "    (\"cent\", \"size\"),\n",
    "    (\"'cent' is a unit of currency, and 'size' is a dimension of physical space or volume.\", \"\"),\n",
    "    (\"chapter\", \"choice\"),\n",
    "    (\"'chapter' refers to a segment of a sequence, often used in the context of books, while 'choice' is about the act of selecting from available options.\", \"\"),\n",
    "    (\"loop\", \"belt\"),\n",
    "    (\"Both relate to circular forms, but different objects.\", \"\"),\n",
    "    (\"sly\", \"tough\"),\n",
    "    (\"Both can describe a person, but with different implications of return.\", \"\"),\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the pairs and comments\n",
    "df = pd.DataFrame(lines, columns=['Word1', 'Word2'])\n",
    "\n",
    "# Save the DataFrame to a TSV file\n",
    "df.to_csv('./results/top_5_difficult.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing explicit representations\n",
    "In this section, we demonstrate the process of computing explicit word embeddings using Latent Semantic Analysis (LSA). The LSA method leverages a co-occurrence matrix and dimensionality reduction via Singular Value Decomposition (SVD) to produce dense word vectors.\n",
    "\n",
    "First of all, to use the previously defined functions (i.e. to compute similarity scores), since our word_vectors is a NumPy array and the functions expect a model with a .wv attribute (like those from Gensim), we need to create a wrapper class that mimics this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for the explicit representation\n",
    "class WordEmbeddingModel:\n",
    "    def __init__(self, word_vectors, vocab):\n",
    "        self.word_vectors = word_vectors\n",
    "        self.vocab = vocab\n",
    "        self.wv = WordVectors(self.word_vectors, self.vocab)\n",
    "\n",
    "class WordVectors:\n",
    "    def __init__(self, word_vectors, vocab):\n",
    "        self.word_vectors = word_vectors\n",
    "        self.vocab = vocab\n",
    "        self.key_to_index = vocab  # Mimicking Gensim's structure\n",
    "\n",
    "    def similarity(self, word1, word2):\n",
    "        if word1 in self.vocab and word2 in self.vocab:\n",
    "            vec1 = self.word_vectors[self.vocab[word1], :]\n",
    "            vec2 = self.word_vectors[self.vocab[word2], :]\n",
    "            return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to construct a co-occurrence matrix that encapsulates the frequency with which words appear in the vicinity of one another within a specified window size throughout the corpus. This matrix is fundamentally large and sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the co-occurrence matrix\n",
    "def create_cooccurrence_matrix(corpus, window_size):\n",
    "    # Create the vocabulary\n",
    "    vocab = set(word for sentence in corpus for word in sentence)\n",
    "    vocab = dict(zip(vocab, range(len(vocab))))\n",
    "    \n",
    "    # Initialize a co-occurrence matrix\n",
    "    cooc_mat = defaultdict(Counter)\n",
    "\n",
    "    # Iterate through the corpus\n",
    "    for sentence in corpus:\n",
    "        # Iterate through the words in the sentence\n",
    "        for i, word in enumerate(sentence):\n",
    "            # Get the start and end indices\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(sentence), i + window_size + 1)\n",
    "            # Iterate through the words in the window\n",
    "            for j in range(start, end):\n",
    "                # Update the co-occurrence matrix\n",
    "                if i != j:\n",
    "                    cooc_mat[vocab[word]][vocab[sentence[j]]] += 1\n",
    "    \n",
    "    # Initialize lists to store the data\n",
    "    data, row_indices, col_indices = [], [], []\n",
    "\n",
    "    # Iterate through the co-occurrence matrix\n",
    "    for word_idx, counts in cooc_mat.items():\n",
    "        # Iterate through the counts\n",
    "        for col_idx, count in counts.items():\n",
    "            # Append the data\n",
    "            data.append(count)\n",
    "            # Append the row and column indices\n",
    "            row_indices.append(word_idx)\n",
    "            # Append the column index\n",
    "            col_indices.append(col_idx)\n",
    "    \n",
    "    # Create a sparse matrix\n",
    "    cooc_sparse = csr_matrix((data, (row_indices, col_indices)), shape=(len(vocab), len(vocab)), dtype=np.float32)\n",
    "    \n",
    "    # Return the sparse matrix and the vocabulary\n",
    "    return cooc_sparse, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply Singular Value Decomposition (SVD) to the co-occurrence matrix. This step reduces the dimensionality of the vectors while maintaining the most significant aspects of the data. We also normalize the vectors to unit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the dimensions of the co-occurrence matrix\n",
    "def reduce_dimensions(cooc_matrix, n_components):\n",
    "    # Using Sparse SVD for dimensionality reduction\n",
    "    u, _, _ = svds(cooc_matrix, k=n_components)\n",
    "    # Normalize the vectors to unit length\n",
    "    u = normalize(u, norm='l2', axis=1)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can retrieve the dense vector representation for any word in our vocabulary by indexing into our matrix of word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NON SEMANTIC score \n",
      "Pearson's r: 0.2145, Spearman's rho: 0.2278\n",
      "\n",
      "SEMANTIC score \n",
      "Pearson's r: 0.2020, Spearman's rho: 0.2067\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a co-occurrence matrix\n",
    "nsm_cooc_matrix, nsm_vocab = create_cooccurrence_matrix(non_semantic_sentences, window_size=5)\n",
    "sm_cooc_matrix, sm_vocab = create_cooccurrence_matrix(semantic_sentences, window_size=5)\n",
    "\n",
    "# Reduce the dimensions of the co-occurrence matrix and return dense word vectors\n",
    "nsm_word_vectors = reduce_dimensions(nsm_cooc_matrix, n_components=100)\n",
    "sm_word_vectors = reduce_dimensions(sm_cooc_matrix, n_components=100)\n",
    "\n",
    "# Create a word embedding model\n",
    "nsm_model_explicit = WordEmbeddingModel(nsm_word_vectors, nsm_vocab)\n",
    "sm_model_explicit = WordEmbeddingModel(sm_word_vectors, sm_vocab)\n",
    "\n",
    "# Compute correlation\n",
    "nsm_exp_pearson_r, nsm_spearman_exp_rho = compute_correlation_scores(nsm_model_explicit, nsm_word_pair2score, mode='non_semantic')\n",
    "sm_exp_pearson_r, sm_spearman_exp_rho = compute_correlation_scores(sm_model_explicit, sm_word_pair2score, mode='semantic')\n",
    "\n",
    "# Print the results\n",
    "print(f\"NON SEMANTIC score \\nPearson's r: {nsm_exp_pearson_r:.4f}, Spearman's rho: {nsm_spearman_exp_rho:.4f}\\n\")\n",
    "print(f\"SEMANTIC score \\nPearson's r: {sm_exp_pearson_r:.4f}, Spearman's rho: {sm_spearman_exp_rho:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
